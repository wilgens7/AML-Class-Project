# -*- coding: utf-8 -*-
"""aml-project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sz1HDCrDjmZcxNrpSis3DlKXFPhavv6Y

## Environment Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# IMPORTS *
# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd           #C ALL COMMENTS
import numpy as np
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split 
import imblearn

import warnings
warnings.filterwarnings('ignore') #SUPPRESSING WARNINGS

"""## Loading Data"""

#Mounting Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load NSL_KDD train dataset
data = pd.read_csv('drive/My Drive/AML-Project/data.csv').iloc[:,:-1]#*
dfkdd_train = data.iloc[:125973,:]

data.head()

"""### Train dataset

### Test dataset
"""

data.head()

"""##Preprocessing Data

### Mapping of attack field to attack class
"""

mapping = {'ipsweep': 'Probe','satan': 'Probe','nmap': 'Probe','portsweep': 'Probe','saint': 'Probe','mscan': 'Probe',
        'teardrop': 'DoS','pod': 'DoS','land': 'DoS','back': 'DoS','neptune': 'DoS','smurf': 'DoS','mailbomb': 'DoS',
        'udpstorm': 'DoS','apache2': 'DoS','processtable': 'DoS',
        'perl': 'U2R','loadmodule': 'U2R','rootkit': 'U2R','buffer_overflow': 'U2R','xterm': 'U2R','ps': 'U2R',
        'sqlattack': 'U2R','httptunnel': 'U2R',
        'ftp_write': 'R2L','phf': 'R2L','guess_passwd': 'R2L','warezmaster': 'R2L','warezclient': 'R2L','imap': 'R2L',
        'spy': 'R2L','multihop': 'R2L','named': 'R2L','snmpguess': 'R2L','worm': 'R2L','snmpgetattack': 'R2L',
        'xsnoop': 'R2L','xlock': 'R2L','sendmail': 'R2L',
        'normal': 'Normal'
        }

# Apply attack class mappings to the dataset
data['attack_class'] = data['attack'].map(mapping)

data.head()

# Drop attack field from both train and test data
data.drop('attack', axis = 'columns', inplace = True)

# Checking data entries 
data.head()

"""### Exploratory Analysis on Data"""

# Descriptive statistics
data.describe()

data.num_outbound_cmds.describe() #*

#Removing 'num_outbound_cmds' feild since it only contains zeros
data.drop('num_outbound_cmds',axis = 'columns', inplace = True)#*

data.head()

#Distribution of target class
from collections import Counter

dist = pd.DataFrame(Counter(data['attack_class']).items())
dist = dist.set_index(dist[0]).drop(0,axis = 'columns')

# dist plot *
plot = dist[[1]].plot(kind="bar");
plot.set_title("Attack Class Distribution", fontsize=20);
plot.grid(color='lightgray', alpha=0.5);

data.head()
data_original = data.copy()#****for famd

"""###Standardization of Numerical Attributes"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

num_cols = data.select_dtypes(include=['float64','int64']).columns
#*
scaled_data = scaler.fit_transform(data.select_dtypes(include=['float64','int64']))


#*
scaled_df = pd.DataFrame(scaled_data,columns = num_cols)

scaled_df

"""### Encoding Categorical Attributes"""

#Do this before above step
from sklearn.preprocessing import LabelEncoder
enc = LabelEncoder()

#selecting all categorical data
categorical_data = data.select_dtypes(include=['object']).copy()

#label encoding categorical data
categorical_data = categorical_data.apply(enc.fit_transform)

categorical_data.head()

"""### Over-Sampling using SMOTE"""

#Defining data and label
X,y = pd.concat([scaled_df,categorical_data],axis='columns').drop('attack_class',axis='columns'),categorical_data['attack_class']

#reference to columns 
columns_reference = pd.concat([scaled_df,categorical_data],axis='columns').columns

#over sampling 
from imblearn.over_sampling import SMOTE # uses KNN to generate new samples
from collections import Counter

#Synthetic Minority Over-Sampling Technique
X_resampled, y_resampled = SMOTE().fit_resample(X,y)

#Splitting test and training data
X_train, X_test, y_train, y_test = train_test_split(X_resampled,y_resampled,test_size = 0.2)

# new distribution of target class

new_dist = Counter(y_resampled).items() #shows that there are equal amounts in each class
new_dist = pd.DataFrame(new_dist,index=['Normal','DoS','Probe','U2R','R2L'])
new_dist

# new dist plot 
plot = new_dist[[1]].plot(kind="bar");
plot.set_title("Attack Class Distribution", fontsize=20);
plot.grid(color='lightgray', alpha=0.5);

"""### Feature Selection"""

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier();

# fitting data 
rfc.fit(X_train, y_train);

# extracting most important features
score = np.round(rfc.feature_importances_,3)
important_features = pd.DataFrame({'feature':pd.concat([scaled_df,categorical_data],axis='columns').drop('attack_class',axis='columns').columns,'importance':score})
important_features = important_features.sort_values('importance',ascending=False).set_index('feature')

# plotting important features
plt.rcParams['figure.figsize'] = (11, 4)
important_features.plot.bar();

#features from random forest classifier 
rf_features = important_features.index[:23] #represents about 90 of data
rf_features

#converting categorical data into numeric type
for i in categorical_data.columns:
  data[i] = categorical_data[i].astype(np.float64)

#!pip3 install prince

#FAMD factor analysis mix data
import prince

data_to_reduce = data_original.drop('attack_class',axis = 'columns')

famd = prince.FAMD(n_components = 15,
                   n_iter = 5,
                   copy = True,
                   check_input = True,
                   engine = 'auto',
                   random_state = 10)
#reference to factor analyzer 
famd = famd.fit(data_to_reduce)

#reference to reduced data
reduced_data = famd.transform(data_to_reduce)

#scatter plot on reduced data
sns.scatterplot(reduced_data[0],reduced_data[1])

# comporing scatter plot with original distributed plot
plot = dist[[1]].plot(kind="bar");
plot.set_title("Attack Class Distribution", fontsize=20);
plot.grid(color='lightgray', alpha=0.5);

#amount of representation of data by each component 
component_contributions = sorted(famd.explained_inertia_,reverse=True)
component_contributions

#90% of the data is contained within the first 10 components
sum(component_contributions[:11])

#plotting 
pd.DataFrame(component_contributions).plot(kind = 'bar')

famd_features = reduced_data

#Feature Selection using correlations
corr_matrix = data.corr()
correlations = pd.DataFrame(corr_matrix['attack_class'].sort_values())
corr_features = correlations[(correlations < - 0.2)|(correlations > 0.2)].dropna()

corr_features

"""### Dataset  Partition"""

#concatinated data
new_data = pd.DataFrame(np.concatenate((X_resampled,y_resampled[:,np.newaxis]),axis = 1),columns = columns_reference)

#data and target for different feature selection techniques 
rf_X = new_data[rf_features]

famd_X = pd.DataFrame(famd_features.iloc[:,:11])

corr_X = new_data[corr_features.index].drop('attack_class',axis = 'columns')

y = new_data['attack_class']

y2 = data_original['attack_class']

"""## Model Selection using cross-val-score"""

#imports for models . 
from sklearn.svm import SVC 
from sklearn.naive_bayes import BernoulliNB 
from sklearn import tree
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

from sklearn import metrics

"""######try_model function"""

def try_model(model, X, y):
  #splitting train and test data
  X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2) 

  # Training  KNN Model
  m = model
  m.fit(X_train, y_train)
 
  print ("Cross_Validation_Mean_Score:""\n",cross_val_score(m,X_train,y_train,cv=5).mean())
  print()
  print (" Accuracy:" "\n",metrics.accuracy_score(y_test, m.predict(X_test)))
  print()
  print("Confusion Matrix:" "\n",metrics.confusion_matrix(y_test, m.predict(X_test)))
  print()
  print("Classification_Report:" "\n", metrics.classification_report(y_test, m.predict(X_test))) 
  print()

"""#####models"""



# Training  KNN Model
knn_classifier = KNeighborsClassifier()
print()
print('==============================KNN Model Evaluation ==============================')
print()

try_model(knn_classifier,corr_X,y)

# Training Logistic Regression Model
logr_classifier = LogisticRegression(n_jobs=-1, random_state=0)

print()
print('==============================Logistic Regression Model Evaluation ==============================')
print()
try_model(logr_classifier,res_corr_X,y)

# Training Gaussian Naive Baye Model
nb_classifier = BernoulliNB()

print()
print('==============================Naive Bayes Model Evaluation ==============================')
print()
try_model(nb_classifier,res_corr_X,y)

# Training Decision Tree Model
dt_classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)

print()
print('==============================Decision Tree Model Evaluation ==============================')
print()
try_model(dt_classifier,res_corr_X,y)

# Training RandomForestClassifier Model
rf_classifier = RandomForestClassifier(criterion='entropy', n_jobs=-1, random_state=0)

print()
print('==============================Random Forest Classifier Model Evaluation ==============================')
print()
try_model(rf_classifier,res_corr_X,y)





"""## Test Models"""

corr_X